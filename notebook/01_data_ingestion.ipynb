{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\Vincent\\\\Desktop\\\\Race-Prediction-Trials\\\\notebook'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\Vincent\\\\Desktop\\\\Race-Prediction-Trials'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3. Update the entity\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataIngestionConfig:\n",
    "    root_dir: Path\n",
    "    constructors_data_path: Path\n",
    "    drivers_data_path: Path\n",
    "    races_data_path: Path\n",
    "    results_data_path: Path\n",
    "    status_data_path: Path\n",
    "    processed_data_path: Path\n",
    "    train_data_path: Path\n",
    "    test_data_path: Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3. Update the entity\n",
    "\n",
    "from src.constants import *\n",
    "from src.utils.common import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4. Update the configuration manager in src config\n",
    "\n",
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self, \n",
    "        config_filepath = CONFIG_FILE_PATH):\n",
    "        \n",
    "        self.config = read_yaml(config_filepath)\n",
    "\n",
    "        create_directories([self.config.output_root])\n",
    "\n",
    "    def get_data_ingestion_config(self) -> DataIngestionConfig:\n",
    "        config = self.config.data_ingestion\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        data_ingestion_config = DataIngestionConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            constructors_data_path=config.constructors_data_path,\n",
    "            drivers_data_path=config.drivers_data_path,\n",
    "            races_data_path=config.races_data_path,\n",
    "            results_data_path=config.results_data_path,\n",
    "            status_data_path=config.status_data_path,\n",
    "            processed_data_path=config.processed_data_path,\n",
    "            train_data_path=config.train_data_path,\n",
    "            test_data_path=config.test_data_path\n",
    "        )\n",
    "\n",
    "        return data_ingestion_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from src.exception import CustomException\n",
    "from src import logger\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 5. Update the components\n",
    "\n",
    "class DataIngestion:\n",
    "    def __init__(self, config: DataIngestionConfig):\n",
    "        self.config = config\n",
    "\n",
    "    def data_preprocessing_feature_engineering(self):\n",
    "        logger.info(\"Data preprocessing and feature engineering\") \n",
    "        \n",
    "        try:\n",
    "            # 1st Dataset \"constructors_mod\"\n",
    "            df_constructor = pd.read_csv(self.config.constructors_data_path)\n",
    "            # Drop the columns 'url', 'constructorRef', 'nationality' which are not useful\n",
    "            df_constructor.drop(columns = ['url', 'constructorRef', 'nationality'], axis=1, inplace=True)\n",
    "            # 2nd Dataset \"results_mod\"\n",
    "            df_result = pd.read_csv(self.config.results_data_path)\n",
    "            # 3rd Dataset \"drivers_mod\"\n",
    "            df_driver = pd.read_csv(self.config.drivers_data_path)\n",
    "            # Extract year in unstructured datetime data 'dob'  \n",
    "            df_driver['yob'] = pd.to_datetime(df_driver['dob']).dt.year\n",
    "            # Apply mathematical Calculations to Features such as addition of 'forename' and 'surname' with an underscore\n",
    "            df_driver['drivername'] = df_driver['forename'] + '_' + df_driver['surname']\n",
    "            # Drop the column 'url', 'forename', 'surname', 'nationality', 'dob', 'driverRef' which are no longer useful\n",
    "            df_driver.drop(columns = ['url', 'code', 'forename', 'surname', 'nationality', 'dob', 'driverRef'], axis=1, inplace=True)\n",
    "            # 4th Dataset \"races_mod\"\n",
    "            df_circuit = pd.read_csv(self.config.races_data_path)\n",
    "            # Extract year in unstructured data 'url text'\n",
    "            df_circuit['year'] = df_circuit['url'].str.extract('(\\d+)')\n",
    "            # Replace NaN value with 2021 in the year column\n",
    "            df_circuit['year'] = df_circuit['year'].replace(np.nan, 2021)\n",
    "            # Replace year 70 to be year 2020 and check again\n",
    "            df_circuit['year'] = df_circuit['year'].replace(['70'], '2020')\n",
    "            # convert the year column datatype to integer\n",
    "            df_circuit['year'] = df_circuit['year'].astype(int)\n",
    "            # Drop the column 'url', 'time', 'date' which are no longer useful\n",
    "            df_circuit.drop(columns = ['url', 'time', 'date'], axis=1, inplace=True)\n",
    "            # 5th Dataset \"status_mod\"\n",
    "            df_status = pd.read_csv(self.config.status_data_path)\n",
    "            # joining df_result with df_driver by column driverID, using inner join\n",
    "            df_result1 = pd.merge(df_result, df_driver, on='driverId')\n",
    "            # joining df_result1 with df_circuit by column raceID, using inner join\n",
    "            df_result2 = pd.merge(df_result1, df_circuit, on='raceId')\n",
    "            # joining df_result2 with df_constructor by column constructorID, using inner join\n",
    "            df_result3 = pd.merge(df_result2, df_constructor, on='constructorId')\n",
    "            # joining df_result3 with df_status by column statusID, using inner join\n",
    "            df_race_finished = pd.merge(df_result3, df_status, on='statusId')\n",
    "            # Drop the columns 'resultId', 'raceId', 'driverId', 'constructorId', 'number_x', 'positionText', 'positionOrder', 'time', 'milliseconds', 'fastestLapTime', 'statusId', 'number_y' and 'circuitId' which are not useful\n",
    "            df_race_finished.drop(columns = ['resultId', 'raceId', 'driverId', 'constructorId', 'number_x', 'positionText', 'positionOrder', 'time', 'milliseconds', 'fastestLapTime', 'statusId', 'number_y', 'circuitId'], axis=1, inplace=True)\n",
    "            # rename the columns accordingly\n",
    "            df_race_finished.rename(columns={'name_x': 'location', 'name_y': 'constructorname'}, inplace=True)\n",
    "            # Applying Mathematical substrations between features 'year' and 'yob' to derive the race_age of the driver\n",
    "            df_race_finished['race_age'] = df_race_finished['year'] - df_race_finished['yob']\n",
    "            df_race_finished.drop(columns = ['yob'], axis=1, inplace=True)\n",
    "            # drop the date and time columns that have no meaningful contributions to modelling\n",
    "            df_race_finished = df_race_finished.drop(columns = ['year', 'fp1_date', 'fp1_time', 'fp2_date', 'fp2_time', 'fp3_date', 'fp3_time', \\\n",
    "                                                                'quali_date', 'quali_time', 'sprint_date', 'sprint_time'])\n",
    "            # replace the '\\N' values with NaN values\n",
    "            df_race_finished.replace('\\\\N', np.NaN, inplace = True)\n",
    "            # convert the columns with the correct datatype \n",
    "            df_race_finished['position'] = df_race_finished['position'].astype(\"Int64\")\n",
    "            df_race_finished['fastestLap'] = df_race_finished['fastestLap'].astype(\"Int64\")\n",
    "            df_race_finished['rank'] = df_race_finished['rank'].astype(\"Int64\")\n",
    "            df_race_finished['fastestLapSpeed'] = df_race_finished['fastestLapSpeed'].astype(float)\n",
    "\n",
    "            os.makedirs(os.path.dirname(self.config.processed_data_path),exist_ok=True)\n",
    "            \n",
    "            df_race_finished.to_csv(self.config.processed_data_path, index=False,header=True)\n",
    "            \n",
    "            return(\n",
    "                self.config.processed_data_path\n",
    "            )\n",
    "\n",
    "        except Exception as e:\n",
    "            raise CustomException(e,sys)    \n",
    "\n",
    "\n",
    "    def complete_data_ingestion(self):\n",
    "        logger.info(\"Resume data ingestion method or component\")\n",
    "        try:\n",
    "            logger.info('Read the dataset as dataframe')\n",
    "\n",
    "            df=pd.read_csv(self.config.processed_data_path)\n",
    "            \n",
    "            logger.info(\"Train test split initiated\")\n",
    "\n",
    "            train_set,test_set=train_test_split(df,test_size=0.3,random_state=42)\n",
    "\n",
    "            os.makedirs(os.path.dirname(self.config.train_data_path),exist_ok=True)\n",
    "\n",
    "            train_set.to_csv(self.config.train_data_path,index=False,header=True)\n",
    "\n",
    "            test_set.to_csv(self.config.test_data_path,index=False,header=True)\n",
    "\n",
    "            logger.info(\"Ingestion of the data is completed\")\n",
    "\n",
    "            return(\n",
    "                self.config.train_data_path,\n",
    "                self.config.test_data_path\n",
    "\n",
    "            )\n",
    "        except Exception as e:\n",
    "            raise CustomException(e,sys)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-07-12 18:33:36,666: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2024-07-12 18:33:36,666: INFO: common: created directory at: output]\n",
      "[2024-07-12 18:33:36,666: INFO: common: created directory at: output/data_ingestion]\n",
      "[2024-07-12 18:33:36,666: INFO: 3527640716: Data preprocessing and feature engineering]\n",
      "[2024-07-12 18:33:36,945: INFO: 3527640716: Resume data ingestion method or component]\n",
      "[2024-07-12 18:33:36,987: INFO: 3527640716: Read the dataset as dataframe]\n",
      "[2024-07-12 18:33:37,090: INFO: 3527640716: Train test split initiated]\n",
      "[2024-07-12 18:33:37,229: INFO: 3527640716: Ingestion of the data is completed]\n"
     ]
    }
   ],
   "source": [
    "## 6. Update the pipeline\n",
    "\n",
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    data_ingestion_config = config.get_data_ingestion_config()\n",
    "    data_ingestion = DataIngestion(config=data_ingestion_config)\n",
    "    data_ingestion.data_preprocessing_feature_engineering()\n",
    "    data_ingestion.complete_data_ingestion()\n",
    "except Exception as e:\n",
    "  raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlparamenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
